{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f568010-fcd9-4f8b-8d31-792b97559cbe",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d60de6b-7ffe-4f1c-bb41-dbe0af01f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import MeanSquaredError,cosine_similarity,MeanAbsoluteError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "from scipy import spatial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88bf9cc-679a-48a4-8e0c-68ebae928019",
   "metadata": {},
   "source": [
    "#### Functions for .h5 Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34d19763-367b-4a48-bca4-245eb73fe758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_data(path):\n",
    "    '''\n",
    "    input: path to .h5 file\n",
    "    output: resamples ADC2 Signal, IMU Signal, data_len = list of all journey length, min_samples= shortest journey (samples), min_index= shortest journey (index)\n",
    "    '''\n",
    "    \n",
    "    # read in file \n",
    "    f = h5py.File(path, 'r')\n",
    "\n",
    "    #create list of all journeys\n",
    "    journey_list = list(f.keys())\n",
    "\n",
    "    #creaty empty dfs for ADC2- and IMU-data \n",
    "    ADC2= pd.DataFrame()\n",
    "    IMU= pd.DataFrame()\n",
    "    data_len = []\n",
    "    \n",
    "    #go through all jounrey and grab IMU and ADC Data\n",
    "    for jrn in journey_list:\n",
    "        \n",
    "        adc_data = f[jrn]['ADC2']['data_ADC2']\n",
    "        imu_data = f[jrn]['IMU']['data_IMU']\n",
    "\n",
    "        #select ch0 = z_acc from ADC Data \n",
    "        df_adc = pd.DataFrame()\n",
    "        for col in adc_data.dtype.names:\n",
    "            df_adc[col] = adc_data[col]         #here df for each journey gets stored\n",
    "        ADC2[jrn] = df_adc['ch0']               #here X gets written column by column\n",
    "\n",
    "        # resample ADC to IMU SampLe Rate\n",
    "        ADC2_res = pd.DataFrame(scipy.signal.resample_poly(ADC2,100 , 20625, axis=0, window=('tukey'), padtype='constant', cval=None))\n",
    "        \n",
    "        #select z_acc from IMU Data \n",
    "        df_imu = pd.DataFrame()\n",
    "        for col in imu_data.dtype.names:\n",
    "            df_imu[col] = imu_data[col]      #here df for each journey gets stored\n",
    "        IMU[jrn] = df_imu['acc_z']           #here y gets written column by column\n",
    "\n",
    "    # count length of jounreys (IMU sample rate) and find minimum index -> other journeys needs to be shortened\n",
    "    for jrn in journey_list:\n",
    "        data_len.append(len(f[jrn]['IMU']['data_IMU']['acc_z'])) # Direct access on the h5-file f\n",
    "    \n",
    "    min_samples = min(data_len)                      # Shorttest timeseries (=journey) in session contains min_sample timepoints\n",
    "    min_index = data_len.index(min_samples)      # min:index = shortest jounrey\n",
    "    \n",
    "    len_adc = len(ADC2_res.iloc[:,0])                \n",
    "    index_list_adc = list(range(min_samples, len_adc)) # list from min_samples to last element in ADC\n",
    "    ADC2_res.drop(ADC2_res.index[index_list_adc], inplace =True) # shorten Dataframe\n",
    " \n",
    "    len_imu = len(IMU.iloc[:,0])\n",
    "    index_list_imu = list(range(min_samples, len_imu))\n",
    "    IMU.drop(IMU.index[index_list_imu], inplace =True)\n",
    "\n",
    "    ADC2_res.columns = IMU.columns        # rename columns in ADC2_res\n",
    "    \n",
    "    return ADC2_res, IMU, data_len, min_samples, min_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51aa2f-fb21-4527-8f0c-7162016f554d",
   "metadata": {},
   "source": [
    "#### Sync Data Function(DataFrame, NP, Sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "837a251e-2e03-4bc4-869d-464041668807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_data_df(ADC,IMU):\n",
    "#input: ADC and IMU as Dataframes (session by session)\n",
    "#output: synced ADC session as pd.DataFrame, sync_log = list of all shifts \n",
    "    # Error handling: wrong dimensions\n",
    "    if IMU.shape != ADC.shape:\n",
    "        print('Error: Dimensions of ADC data and IMU data are not matching')\n",
    "        sys.exit()\n",
    "        \n",
    "    ADC_sync= pd.DataFrame()\n",
    "    sync_log = []\n",
    "    for column in ADC:\n",
    "        ADC_ = ADC[column]\n",
    "        IMU_ = IMU[column]\n",
    "        corr = np.correlate(IMU_ , ADC_ , mode='same')\n",
    "        delta = (len(corr)/2) - np.argmax(np.abs(corr))\n",
    "        sync_log.append(delta)\n",
    "        \n",
    "        # create ADC_sync by padding with zeros and cutting of delta elements\n",
    "        if delta > 0: # if delta positive, zeros will be podded at the end -> points at the beginning will be cut off\n",
    "            ADC_sync_ = np.append(ADC_,np.zeros(int(delta)))[int(delta):]\n",
    "        elif delta < 0: # if delta negaitve, zeros will be podded at the beginning -> points at the end will be cut off\n",
    "            ADC_sync_ = np.append(np.zeros(int(np.abs(delta))),ADC_)[:-int(np.abs(delta))]\n",
    "        elif delta == 0:\n",
    "            ADC_sync_ = ADC_\n",
    "            \n",
    "        ADC_sync[column] = pd.DataFrame(ADC_sync_)\n",
    "        \n",
    "    return ADC_sync, sync_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd7f84a-30d6-43be-ab92-7fcaec2e818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_data_np(ADC,IMU):\n",
    "#input: ADC and IMU as np.arrays (journey by journey)\n",
    "#output: synced ADC journey as np.array, delta = shift\n",
    "    # cross correlation of ADC and IMU Signal\n",
    "    corr = np.correlate(IMU , ADC , mode='same')\n",
    "\n",
    "    # Delta = value to shift ADC in order to sync with ADC signal\n",
    "    # When signals are in sync, delta is 0 because peak of correlation functions is in the middle of the timeseries\n",
    "    delta = (len(corr)/2) - np.argmax(np.abs(corr))\n",
    "    delta_int = int(np.abs(delta))\n",
    "                                     \n",
    "    \n",
    "    # create ADC_sync by padding with zeros and cutting of delta elements\n",
    "    if delta > 0: # if delta positive, zeros will be podded at the end -> points at the beginning will be cut off\n",
    "        \n",
    "        ADC_sync = np.pad(ADC[delta_int:],((0,delta_int)),'edge')\n",
    "        #ADC_sync = np.append(ADC,np.zeros(int(delta)))[int(delta):]\n",
    "    elif delta < 0: # if delta negaitve, zeros will be podded at the beginning -> points at the end will be cut off\n",
    "        ADC_sync = np.pad(ADC[:int(len(ADC)+delta)],((delta_int,0)),'edge')\n",
    "        #ADC_sync = np.append(np.zeros(int(np.abs(delta))),ADC)[:-int(np.abs(delta))]\n",
    "    elif delta == 0:\n",
    "        ADC_sync = ADC\n",
    "        \n",
    "    return ADC_sync, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eee01ccd-8bc6-4c65-8a2d-0a2aed7d2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_sparse(adc_sp,imu_sp,mx):\n",
    "#input: ADC and IMU as sparse arrays (session by session)\n",
    "#output: synced ADC session as sparse, deltas = list of all shifts \n",
    "    # grab first journey\n",
    "    adc_first = adc_sp[0].data\n",
    "    imu_first = imu_sp[0].data\n",
    "    deltas = []\n",
    "\n",
    "    # ceate first row by hand\n",
    "    sync, delta = sync_data_np(adc_first, imu_first)\n",
    "    new_adc = pad_zero(sync, mx)\n",
    "\n",
    "    for jrn in range(adc_sp.shape[0]-1):\n",
    "        sync, delta = sync_data_np(adc_sp[jrn+1].data, imu_sp[jrn+1].data)\n",
    "        new_row = pad_zero(sync,mx)\n",
    "        new_adc = np.vstack((new_adc, new_row))\n",
    "        deltas = np.append(deltas,delta)\n",
    "\n",
    "    # create sparse array from array                 \n",
    "    adc_sync_sp = csr_matrix(new_adc)\n",
    "    return adc_sync_sp, deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d27e59-628a-4619-938a-3d2bdf898ca8",
   "metadata": {},
   "source": [
    "#### Filter Function (NP, Sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2aa4d8c-c65c-4142-a5cb-5202c16ba26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Bandpass filter\n",
    "from scipy.signal import butter, lfilter, sosfiltfilt\n",
    "\n",
    "# define function for creating filter object\n",
    "def butter_bandpass(lowcut, highcut, fs, order):    \n",
    "#input: lowcut and highcut frequendy of the filter, fs = sampling rate, order = filter order\n",
    "# output: butterworth filter parameters\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high],output='sos',btype='band')\n",
    "    return sos\n",
    "\n",
    "# define function to apply created filter to data\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order):\n",
    "# input: data = signal that gets filtered, lowcut and highcut frequendy of the filter, fs = sampling rate, order = filter order\n",
    "# output: y = filtered signal\n",
    "    sos = butter_bandpass(lowcut, highcut, fs, order)\n",
    "    y = sosfiltfilt(sos, data)\n",
    "    return y\n",
    "\n",
    "def filt_sparse(adc,imu,low, high_a, high_i, fs, mx, ordn):\n",
    "# input: adc and imu as sparse arrays (session by session), high_a/i = highcut freq for aba/imu, low = lowcut freq for both signals, mx = longest journey samples in this session, ordn = filter order\n",
    "# output: filtered sparse arrays \n",
    "    # create first row of array\n",
    "    # in Order to create Sparse elemtn in the end, i need to properly insert first element and start iteration from 2nd element on\n",
    "    adc_filt = pad_zero(butter_bandpass_filter(adc[0].data, low, high_a, fs, order=ordn), mx)\n",
    "    imu_filt = pad_zero(butter_bandpass_filter(imu[0].data, low, high_i, fs, order=ordn), mx)\n",
    "\n",
    "    # fill up array with all jrn of the session\n",
    "    for jrn in range(adc.shape[0]-1):\n",
    "        new_adc = butter_bandpass_filter(adc[jrn+1].data, low, high_a, fs, order=2)\n",
    "        new_imu = butter_bandpass_filter(imu[jrn+1].data, low, high_i, fs, order=2)\n",
    "        adc_filt = np.vstack((adc_filt, pad_zero(new_adc, mx)))\n",
    "        imu_filt = np.vstack((imu_filt, pad_zero(new_imu, mx)))\n",
    "\n",
    "    # create sparse array from array                 \n",
    "    adc_f = csr_matrix(adc_filt)\n",
    "    imu_f = csr_matrix(imu_filt)\n",
    "\n",
    "    return adc_f,imu_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93512b-21d2-4a55-ab76-940142d3146d",
   "metadata": {},
   "source": [
    "#### Quarter Car Function (from B.B.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11efd5c4-f281-4009-b525-5f1517e8163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_car(f, m_s, m_us, k_s, k_us, c):\n",
    "    \"\"\"\n",
    "    This function models the impulse response of a \n",
    "    single degree-of freedom oscillator in the frequency domain.\n",
    "    Formula is taken form Schenkendorf et al. (2016; \"Improved Railway Track \n",
    "    Irregularities Classification by a Model Inversion Approach\"; Formulas (4-8))\n",
    "    \"\"\"\n",
    "    #initialisation:\n",
    "    w = 2*np.pi*f\n",
    "    A = np.empty((4,4))\n",
    "    B = np.empty((4,1))\n",
    "    C = np.empty((1,4))\n",
    "    G = np.empty((len(f)),dtype=complex)\n",
    "    #\n",
    "    A[0:4,0] =[-c/m_s, c/m_us, 1, 0]\n",
    "    A[0:4,1] =[c/m_s, -c/m_us, 0, 1]\n",
    "    A[0:4,2] =[-k_s/m_s, k_s/m_us, 0, 0]\n",
    "    A[0:4,3] =[k_s/m_s, -(k_s+k_us)/m_us, 0, 0]\n",
    "    \n",
    "    B[0:4,0] = [0, k_us/m_us, 0, 0]\n",
    "    \n",
    "    C[0,0:4] = [-c/m_s, c/m_s, -k_s/m_s, k_s/m_s]\n",
    "    D = 0\n",
    "#    C[0,0:4] = [c/m_us, -c/m_us, k_s/m_us, -(k_s+k_us)/m_us]\n",
    "#    D = k_us/m_us\n",
    "    for wc, ww in enumerate(w):\n",
    "        G[wc] = (np.matmul(np.matmul(C,np.linalg.inv(1j*ww*np.identity(4) - A)),B)+D)[0]\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8cedd-edd4-4970-bb5a-b39b305ed024",
   "metadata": {},
   "source": [
    "#### Zero Padding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63a8311-85ba-460f-b024-d535c2d6514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zero(arr, mx):\n",
    "# input: np.array, mx = desired array size \n",
    "# output: padded np.array\n",
    "    delta = mx-arr.size\n",
    "    #new_arr = [arr, np.zeros(delta)]\n",
    "    new_arr = np.concatenate((arr, np.zeros(delta)))\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228af2c2-1b4c-4404-af67-8e6e35014610",
   "metadata": {},
   "source": [
    "#### Equalize length function (NP, sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac594b9c-4f8f-4cab-b52a-6e9ff9e3b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq_adc_imu_np(adc, imu):\n",
    "# input: adc and imu signal as np.arrays (journey by journey)\n",
    "# output: length equalised aba signal\n",
    "    \n",
    "    adcsize = adc.size\n",
    "    imusize = imu.size   \n",
    "    pd =  imusize-adcsize # pad width\n",
    "\n",
    "    if pd <= 0:\n",
    "        adcpd = adc[-pd:]       \n",
    "        \n",
    "    else:\n",
    "    \n",
    "        if pd%2==0: \n",
    "            adcpd = np.pad(adc, ((int(pd/2), int(pd/2))), 'edge')\n",
    "        elif pd%2==1:\n",
    "            adcpd = np.pad(adc, ((int(np.floor(pd/2)), int(np.floor(pd/2)+1))), 'edge')\n",
    "\n",
    "    return adcpd\n",
    "\n",
    "def eq_adc_imu_sparse(adc_sp,imu_sp, mx):\n",
    "# input: aba and imu signals as sparse arrays (session by session), mx = longest journey duration in samples in this session\n",
    "# output: length equalised aba signals\n",
    "    \n",
    "    # takes sparse array as input and creates new sparse array for adc with equal size \n",
    "    adc_first = adc_sp[0].data\n",
    "    imu_first = imu_sp[0].data\n",
    "    # create first row of array\n",
    "    # in Order to create Sparse elemtn in the end, i need to properly insert first element and start iteration from 2nd element on\n",
    "    #pd = pad_zero(eq_adc_imu_np(s4_adc_f[12].data,s4_imu_f[12].data),mx4)\n",
    "    new_adc = pad_zero(eq_adc_imu_np(adc_first,imu_first), mx)\n",
    "\n",
    "    for jrn in range(adc_sp.shape[0]-1):\n",
    "        delta = imu_sp[jrn+1].data.size-adc_sp[jrn+1].data.size\n",
    "        if delta <0:\n",
    "            new_row = pad_zero(adc_sp[jrn+1].data[:delta],mx)\n",
    "        else:\n",
    "            new_row = pad_zero(eq_adc_imu_np(adc_sp[jrn+1].data, imu_sp[jrn+1].data),mx)\n",
    "\n",
    "        new_adc = np.vstack((new_adc, new_row))\n",
    "\n",
    "    # create sparse array from array                 \n",
    "    adc_eq_sp = csr_matrix(new_adc)\n",
    "    return adc_eq_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e0538-255c-4966-b0c7-e0760ef007d1",
   "metadata": {},
   "source": [
    "#### Scaling function (NP, sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e59368a5-d28c-454d-92fe-35502a74793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_np(adc, imu):\n",
    "# standard scale\n",
    "# input: aba and imu signal as np.array\n",
    "# output: scaled signals as np.array\n",
    "    scaler_adc = StandardScaler().fit(adc.reshape(-1, 1))\n",
    "    adc_sc = scaler_adc.transform(adc.reshape(-1, 1))\n",
    "\n",
    "    scaler_imu = StandardScaler().fit(imu.reshape(-1, 1))\n",
    "    imu_sc = scaler_imu.transform(imu.reshape(-1, 1))\n",
    "\n",
    "    return adc_sc.reshape(-1),imu_sc.reshape(-1)\n",
    "\n",
    "def scale_sparse(adc_sp,imu_sp, mx):\n",
    "# standard scale\n",
    "# input: aba and imu signal as sparse arrays\n",
    "# output: scaled aba and imu signal as sparse arrays\n",
    "    # takes sparse array as input and creates new sparse array for adc with equal size \n",
    "    adc_first = adc_sp[0].data\n",
    "    imu_first = imu_sp[0].data\n",
    "    # create first row of array\n",
    "    # in Order to create Sparse elemtn in the end, i need to properly insert first element and start iteration from 2nd element on\n",
    "    #pd = pad_zero(eq_adc_imu_np(s4_adc_f[12].data,s4_imu_f[12].data),mx4)\n",
    "    adc_first_sc, imu_first_sc = scale_np(adc_first,imu_first)\n",
    "    new_adc = pad_zero(adc_first_sc, mx)\n",
    "    new_imu = pad_zero(imu_first_sc, mx)\n",
    "\n",
    "    for jrn in range(adc_sp.shape[0]-1):\n",
    "        adc_sc, imu_sc = scale_np(adc_sp[jrn+1].data,imu_sp[jrn+1].data)\n",
    "        new_adc_row = pad_zero(adc_sc, mx)\n",
    "        new_imu_row = pad_zero(imu_sc, mx)\n",
    "        \n",
    "        new_adc = np.vstack((new_adc, new_adc_row))\n",
    "        new_imu = np.vstack((new_imu, new_imu_row))\n",
    "\n",
    "    # create sparse array from array                 \n",
    "    adc_sc_sp = csr_matrix(new_adc)\n",
    "    imu_sc_sp = csr_matrix(new_imu)\n",
    "    return adc_sc_sp, imu_sc_sp\n",
    "\n",
    "\n",
    "def scale_np_max(adc, imu):\n",
    "# MinMax Scale\n",
    "# input: aba and imu signal as np.array\n",
    "# output: scaled signals as np.array\n",
    "    scaler_adc = MinMaxScaler().fit(adc.reshape(-1, 1))\n",
    "    adc_sc = scaler_adc.transform(adc.reshape(-1, 1))\n",
    "\n",
    "    scaler_imu = StandardScaler().fit(imu.reshape(-1, 1))\n",
    "    imu_sc = scaler_imu.transform(imu.reshape(-1, 1))\n",
    "\n",
    "    return adc_sc.reshape(-1),imu_sc.reshape(-1)\n",
    "\n",
    "def scale_sparse_max(adc_sp,imu_sp, mx):\n",
    "# MinMax Scale\n",
    "# input: aba and imu signal as sparse arrays\n",
    "# output: scaled aba and imu signal as sparse arrays\n",
    "    # takes sparse array as input and creates new sparse array for adc with equal size \n",
    "    adc_first = adc_sp[0].data\n",
    "    imu_first = imu_sp[0].data\n",
    "    # create first row of array\n",
    "    # in Order to create Sparse elemtn in the end, i need to properly insert first element and start iteration from 2nd element on\n",
    "    #pd = pad_zero(eq_adc_imu_np(s4_adc_f[12].data,s4_imu_f[12].data),mx4)\n",
    "    adc_first_sc, imu_first_sc = scale_np_max(adc_first,imu_first)\n",
    "    new_adc = pad_zero(adc_first_sc, mx)\n",
    "    new_imu = pad_zero(imu_first_sc, mx)\n",
    "\n",
    "    for jrn in range(adc_sp.shape[0]-1):\n",
    "        adc_sc, imu_sc = scale_np_max(adc_sp[jrn+1].data,imu_sp[jrn+1].data)\n",
    "        new_adc_row = pad_zero(adc_sc, mx)\n",
    "        new_imu_row = pad_zero(imu_sc, mx)\n",
    "        \n",
    "        new_adc = np.vstack((new_adc, new_adc_row))\n",
    "        new_imu = np.vstack((new_imu, new_imu_row))\n",
    "\n",
    "    # create sparse array from array                 \n",
    "    adc_sc_sp = csr_matrix(new_adc)\n",
    "    imu_sc_sp = csr_matrix(new_imu)\n",
    "    return adc_sc_sp, imu_sc_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55201f1a-e539-4d51-9c7a-b26969bf13a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Convert to SparseTensor \n",
    "(from: https://stackoverflow.com/questions/40896157/scipy-sparse-csr-matrix-to-tensorflow-sparsetensor-mini-batch-gradient-descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78a9814c-ecf6-45b8-b005-ae4aba16204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf3482-b7b7-4039-b09c-835267a1318f",
   "metadata": {},
   "source": [
    "### window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0da306-1419-4d3b-bd00-79bd0f19c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Window Dataset \n",
    "# Input format: np.array\n",
    "def arr_to_window(X, y, window):\n",
    "# input: 2 signals X and y as np.arrays , desired window size\n",
    "# output: 2 new arrays that contain structured data with desired window size \n",
    "    X_win = []\n",
    "    y_win = []\n",
    "    samples = len(X)\n",
    "    for i in range(int(samples/window)):\n",
    "        i = i*window \n",
    "        row_x = [[a] for a in X[i:i+window]] # adding in korrekt format!\n",
    "        row_y = [[a] for a in y[i:i+window]] # adding in korrekt format!\n",
    "        X_win.append(row_x)\n",
    "        y_win.append(row_y)\n",
    "            \n",
    "    return np.array(X_win), np.array(y_win)\n",
    "    \n",
    "# \n",
    "def sparse_to_window(X_sp, y_sp, window):\n",
    "# input: sparse arrays X_sp and y_sp, desired window size\n",
    "# output: 2 new sparse arrays that contain structured data with desired window size \n",
    "    jrns = X_sp.shape[0]\n",
    "    X_win = []\n",
    "    y_win = []\n",
    "    for jrn in range(jrns):\n",
    "        X_ = X_sp[jrn].data    \n",
    "        y_ = y_sp[jrn].data\n",
    "        samples = len(X_)\n",
    "        for i in range(int(np.floor(samples/window))):\n",
    "            i = i*window\n",
    "            row_x = [[a] for a in X_[i:i+window]] # adding in korrekt format!\n",
    "            row_y = [[a] for a in y_[i:i+window]] # adding in korrekt format!\n",
    "            X_win.append(row_x)\n",
    "            y_win.append(row_y)\n",
    "            \n",
    "    return np.array(X_win), np.array(y_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4258d35-63ba-40c3-b9f2-97cabbf52b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(x_sp, y_sp):\n",
    "# input: sparse arrays x_sp and y_sp\n",
    "# output: Dynamic Time Warp Score, Pearson Correlation Coefficient, Cosine Similarity for all arrays in this sparse arrays/ sessions in this journey\n",
    "    from tslearn.metrics import dtw, dtw_path\n",
    "    from scipy.stats import pearsonr\n",
    "    DTW = []\n",
    "    CosSin = []\n",
    "    Pear = []\n",
    "    \n",
    "    for jrn in range(x_sp.shape[0]):\n",
    "\n",
    "        # load journey data\n",
    "        x = x_sp[jrn].data.astype(float)\n",
    "        y = y_sp[jrn].data.astype(float)\n",
    "\n",
    "        # DTW Score\n",
    "        if len(x)>50000:\n",
    "            DTW.append(int(100))\n",
    "        else: \n",
    "            DTW.append(dtw(x, y))\n",
    "\n",
    "        # Cosine Similarity\n",
    "        CosSin.append(np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y)))\n",
    "\n",
    "        # Pearson Coefficient\n",
    "        corr, pvalue = pearsonr(x,y)\n",
    "        Pear.append(corr)\n",
    "        \n",
    "\n",
    "    return DTW,Pear,CosSin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
